from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langsmith.evaluation import evaluate, LangChainStringEvaluator
from langsmith.schemas import Run, Example
from openai import OpenAI
import json

from dotenv import load_dotenv
load_dotenv()

from langsmith.wrappers import wrap_openai
from langsmith import traceable

client = wrap_openai(OpenAI())

@traceable
def prompt_compliance_evaluator(run: Run, example: Example) -> dict:
    inputs = example.inputs['input']
    outputs = example.outputs['output']

    # Extract system prompt
    system_prompt = next((msg['data']['content'] for msg in inputs if msg['type'] == 'system'), "")

    # Extract message history
    message_history = []
    for msg in inputs:
        if msg['type'] in ['human', 'ai']:
            message_history.append({
                "role": "user" if msg['type'] == 'human' else "assistant",
                "content": msg['data']['content']
            })

    # Extract latest user message and model output
    latest_message = message_history[-1]['content'] if message_history else ""
    model_output = outputs['data']['content']

    evaluation_prompt = f"""
    System Prompt: {system_prompt}

    Model Output: {model_output}

    You are an expert evaluator tasked with assessing the quality of a CliffsNotes-style summary generated by an LLM. Your goal is to evaluate both the title and the content of the model's summary based on the following criteria:

	1.	Title Evaluation
	•	Relevance: Does the title accurately reflect the main topic and purpose of the document?
	•	Clarity: Is the title clear, concise, and easy to understand?
	•	Conciseness: Does the title avoid unnecessary words while still conveying the essence of the document?

    Please provide your assessment of the title based on these criteria, along with a rating from 1 to 5, where 1 is poor and 5 is excellent.

	2.	Content Evaluation
	•	Coverage:
	•	How well does the summary capture the key themes, main points, and critical details from the original document?
	•	Does the summary omit any significant information?
	•	Conciseness:
	•	Is the summary succinct, avoiding redundancy and unnecessary detail?
	•	Is the summary's length appropriate relative to the original document, providing essential information without being overly verbose?
	•	Clarity:
	•	Is the summary well-organized, with ideas presented in a logical and coherent manner?
	•	Is the language used clear and easy to understand?

    Please provide your assessment of the content based on these criteria, along with a rating from 1 to 5, where 1 is poor and 5 is excellent.

    Respond in the following JSON format with no spaces:
    {{
        "explanation": "<string>",
        "title_score": <int>,
        "content_score": <int>
    }}
    """

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are an AI assistant tasked with evaluating the compliance of model outputs to given prompts and conversation context."},
            {"role": "user", "content": evaluation_prompt}
        ],
        temperature=0.2
    )

    try:
        result = json.loads(response.choices[0].message.content)
        return {
            "key": "prompt_compliance",
            "score": (result["title_score"]+result["content_score"]) / 10,  # Normalize to 0-1 range
            "reason": result["explanation"]
        }
    except json.JSONDecodeError:
        return {
            "key": "prompt_compliance",
            "score": 0,
            "reason": "Failed to parse evaluator response"
        }

# The name or UUID of the LangSmith dataset to evaluate on.
data = "doc_summarizer"

# A string to prefix the experiment name with.
experiment_prefix = "Document summarizer prompt compliance"

# List of evaluators to score the outputs of target task
evaluators = [
    prompt_compliance_evaluator
]

# Evaluate the target task
results = evaluate(
    lambda inputs: inputs,
    data=data,
    evaluators=evaluators,
    experiment_prefix=experiment_prefix,
)

print(results)